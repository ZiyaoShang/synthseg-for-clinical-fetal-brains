"""

This script shows how we trained SynthSeg.
Importantly, it reuses numerous parameters seen in the previous tutorial about image generation
(i.e., 2-generation_explained.py), which we strongly recommend reading before this one.



If you use this code, please cite one of the SynthSeg papers:
https://github.com/BBillot/SynthSeg/blob/master/bibtex.bib

Copyright 2020 Benjamin Billot

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in
compliance with the License. You may obtain a copy of the License at
https://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is
distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied. See the License for the specific language governing permissions and limitations under the
License.
"""
print("start")
import os
import sys
import numpy as np
import tensorflow as tf
# print("imported tf")
# project imports
from SynthSeg.training import training
from SynthSeg.validate import validate_training, plot_validation_curves
# from scripts.fetal_scripts.predict_func import pred_all

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# model name 
exp_name = "r2_exp2"
# path to training label maps
path_training_label_maps = '/cluster/work/menze/zshang/synthseg_data/zurich/all_extra_label_centered_seg_train'
# path to save all model checkpoints
path_model_dir = '/cluster/work/menze/zshang/experiments/model/' + exp_name


# architecture parameters  
n_levels = 5           # number of resolution levels
nb_conv_per_level = 2  # number of convolution per level
conv_size = 3          # size of the convolution kernel (e.g. 3x3x3)
unet_feat_count = 24   # number of feature maps after the first convolution
activation = 'elu'     # activation for all convolution layers except the last, which will use softmax regardless
feat_multiplier = 2    # if feat_multiplier is set to 1, we will keep the number of feature maps constant throughout the network; 2 will double them(resp. half) after each max-pooling (resp. upsampling); 3 will triple them, etc.

# training parameters
batchsize = 1           # batch size
lr = 1e-4               # learning rate
wl2_epochs = 1          # number of pre-training epochs with wl2 metric w.r.t. the layer before the softmax IMPORTANT: set wl2_epochs to zero if using checkpoint !!!!!
dice_epochs = 20        # number of training epochs
steps_per_epoch = 5000  # number of iteration per epoch
checkpoint = None

# ---------- Generation parameters ----------
# these parameters are from the previous tutorial, and thus we do not explain them again here

# generation and segmentation labels (in my case, the first five are all background labels, which gets converted to zero) after the Synthseg generator 
path_generation_labels = np.array([0,10,11,12,13,1,2,3,4,5,6,7])
n_neutral_labels = 12
path_segmentation_labels = np.array([0,0,0,0,0,1,2,3,4,5,6,7])

# shape and resolution of the outputs
target_res = None
output_shape = 160
# important!!! output_div_by_n == 2 ** n_levels
n_channels = 1

# GMM sampling
prior_distributions = 'uniform'
path_generation_classes = None

# spatial deformation parameters
flipping = False
scaling_bounds = 0.2
rotation_bounds = 20
shearing_bounds = .012
translation_bounds = False
nonlin_std = 4.
bias_field_std = .7

# acquisition resolution parameters
randomise_res = True

inflate = False # please set to False (new feature not throughly tested yet)

# ------------------------------------------------------ validation params  ------------------------------------------------------
# path for saving the outputs when validating the model
val_dir = os.path.join("/cluster/work/menze/zshang/validation/", exp_name)
# validation image and ground truth directories
val_img_dir = '/cluster/work/menze/zshang/data/ZURICH/original_data/mri_train'
val_gt_dir = '/cluster/work/menze/zshang/data/ZURICH/original_data/seg_train'

# ------------------------------------------------------ weighted sampling  ------------------------------------------------------
subjects_prob = None
weighted_sampling = True
if weighted_sampling: # if needed, please enter the sampling weights generated by feature_extractor.py
    subjects_prob = "/cluster/work/menze/zshang/synthseg_data/zurich/features_weights/zurich_clex1_noinf_origbg/zurich_train_weights.npy"
    print(f"we now use weighted sampling... using the weights from {subjects_prob}")

# ------------------------------------------------------ Training ------------------------------------------------------

# crate model directory if needed
assert not os.path.isdir(val_dir)
os.mkdir(val_dir)
assert path_model_dir.split("/")[-1] == exp_name
assert not os.path.isdir(path_model_dir)
os.mkdir(path_model_dir)

# start training
print("experiment name " + exp_name)
print("label maps from " + path_training_label_maps)
print("madels saved in " + path_model_dir)
print("validation main dir " + val_dir)

print("__commence trianing__")
training(path_training_label_maps,
         path_model_dir,
         generation_labels=path_generation_labels,
         segmentation_labels=path_segmentation_labels,
         subjects_prob=subjects_prob,
         n_neutral_labels=n_neutral_labels,
         batchsize=batchsize,
         n_channels=n_channels,
         target_res=target_res,
         output_shape=output_shape,
         prior_distributions=prior_distributions,
         generation_classes=path_generation_classes,
         flipping=flipping,
         scaling_bounds=scaling_bounds,
         rotation_bounds=rotation_bounds,
         shearing_bounds=shearing_bounds,
         translation_bounds=translation_bounds,
         nonlin_std=nonlin_std,
         randomise_res=randomise_res,
         bias_field_std=bias_field_std,
         n_levels=n_levels,
         nb_conv_per_level=nb_conv_per_level,
         conv_size=conv_size,
         unet_feat_count=unet_feat_count,
         feat_multiplier=feat_multiplier,
         activation=activation,
         lr=lr,
         wl2_epochs=wl2_epochs,
         dice_epochs=dice_epochs,
         steps_per_epoch=steps_per_epoch,
         checkpoint=checkpoint,
         inflate=inflate)

# ------------------------------------------------------ Validation ------------------------------------------------------

print("__commence validation__")
validate_training(image_dir=val_img_dir,
                    gt_dir=val_gt_dir,
                    models_dir=path_model_dir,
                    validation_main_dir=val_dir,
                    labels_segmentation=[0,1,2,3,4,5,6,7],
                    n_neutral_labels=8,
                    evaluation_labels=[0,1,2,3,4,5,6,7],
                    step_eval=1,
                    min_pad=None,
                    cropping=None,
                    target_res=None,
                    gradients=False,
                    flip=False,
                    topology_classes=None,
                    sigma_smoothing=0.5,
                    keep_biggest_component=True,
                    n_levels=5,
                    nb_conv_per_level=2,
                    conv_size=3,
                    unet_feat_count=24,  # original: 24
                    feat_multiplier=2,
                    activation='elu',
                    recompute=False)

max_epoch = plot_validation_curves(list_validation_dirs=np.array([val_dir]), architecture_names=[exp_name], eval_indices=np.array([1,2,3,4,5,6,7]), skip_first_dice_row=True, size_max_circle=100, figsize=(11, 6), y_lim=None, fontsize=18, list_linestyles=None, list_colours=None, plot_legend=False, draw_line=None)

print("the best epoch: " + str(max_epoch))
if int(max_epoch) < 12:
    print("Note: the best epoch is less than 12!")


